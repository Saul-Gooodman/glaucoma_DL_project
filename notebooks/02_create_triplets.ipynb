{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b55d460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: /Users/v/Desktop/Visual/glaucoma_DL_project/data/uwhvf-master/CSV/VF_Data.csv\n",
      "Dataset loaded: (28943, 184)\n",
      "Columns: 184\n",
      "Unique patients: 3871\n",
      "Unique eyes: 7428\n",
      "Triplets created: 14117\n",
      "Saved triplets summary to: /Users/v/Desktop/Visual/glaucoma_DL_project/data/processed/triplets_summary.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "# 1. Set up paths\n",
    "\n",
    "# Project root (modify if needed)\n",
    "project_root = Path(\"/Users/v/Desktop/Visual/glaucoma_DL_project\")\n",
    "data_path = project_root / \"data\" / \"uwhvf-master\" / \"CSV\" / \"VF_Data.csv\"\n",
    "output_path = project_root / \"data\" / \"processed\"\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Loading data from: {data_path}\")\n",
    "\n",
    "# 2. Load and inspect data\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "print(\"Dataset loaded:\", df.shape)\n",
    "print(\"Columns:\", len(df.columns))\n",
    "print(\"Unique patients:\", df['PatID'].nunique())\n",
    "print(\"Unique eyes:\", df[['PatID','Eye']].drop_duplicates().shape[0])\n",
    "\n",
    "# Sort to make sure each eye’s records are ordered\n",
    "df = df.sort_values(by=[\"PatID\", \"Eye\", \"FieldN\"]).reset_index(drop=True)\n",
    "\n",
    "# 3. Construct triplets (v1, v2 -> v3)\n",
    "\n",
    "triplets = []\n",
    "\n",
    "for (pid, eye), group in df.groupby([\"PatID\", \"Eye\"]):\n",
    "    group = group.sort_values(\"FieldN\")\n",
    "    \n",
    "    # skip if fewer than 3 visits\n",
    "    if len(group) < 3:\n",
    "        continue\n",
    "    \n",
    "    # slide over consecutive visits\n",
    "    for i in range(len(group) - 2):\n",
    "        v1 = group.iloc[i]\n",
    "        v2 = group.iloc[i+1]\n",
    "        v3 = group.iloc[i+2]\n",
    "        \n",
    "        triplets.append({\n",
    "            \"PatID\": pid,\n",
    "            \"Eye\": eye,\n",
    "            \"v1_FieldN\": v1[\"FieldN\"],\n",
    "            \"v2_FieldN\": v2[\"FieldN\"],\n",
    "            \"v3_FieldN\": v3[\"FieldN\"],\n",
    "            \"Age_v1\": v1[\"Age\"],\n",
    "            \"Age_v2\": v2[\"Age\"],\n",
    "            \"Age_v3\": v3[\"Age\"],\n",
    "            \"MTD_v1\": v1[\"MTD\"],\n",
    "            \"MTD_v2\": v2[\"MTD\"],\n",
    "            \"MTD_v3\": v3[\"MTD\"],\n",
    "            \"PSD_v1\": v1[\"PSD\"],\n",
    "            \"PSD_v2\": v2[\"PSD\"],\n",
    "            \"PSD_v3\": v3[\"PSD\"]\n",
    "        })\n",
    "\n",
    "triplet_df = pd.DataFrame(triplets)\n",
    "print(\"Triplets created:\", len(triplet_df))\n",
    "triplet_df.head(10)\n",
    "\n",
    "# 4. Save triplets for later model training\n",
    "\n",
    "save_path = output_path / \"triplets_summary.csv\"\n",
    "triplet_df.to_csv(save_path, index=False)\n",
    "print(\"Saved triplets summary to:\", save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d71e12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading VF data: /Users/v/Desktop/Visual/glaucoma_DL_project/data/uwhvf-master/CSV/VF_Data.csv\n",
      "Loading coordinates: /Users/v/Desktop/Visual/glaucoma_DL_project/data/uwhvf-master/CSV/Coord_242.csv\n",
      "Detected 54 TD columns total.\n",
      "Blind-spot TD columns to exclude: ['TD_26', 'TD_35']\n",
      "Using 52 TD locations after removing blind spots.\n",
      "Built X shape: (14117, 2, 52)  (N samples, 2 channels, 52 locations)\n",
      "Built Y shape: (14117, 52)  (N samples, 52 locations)\n",
      "Meta rows: (14117, 14)\n",
      "Saved:\n",
      "  X -> /Users/v/Desktop/Visual/glaucoma_DL_project/data/features/X_td2ch_L52.npy\n",
      "  Y -> /Users/v/Desktop/Visual/glaucoma_DL_project/data/features/Y_td_L52.npy\n",
      "  meta -> /Users/v/Desktop/Visual/glaucoma_DL_project/data/processed/triplets_meta_L52.csv\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# Build model-ready X/Y from TD_1–54\n",
    "# ================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Paths & settings\n",
    "# -----------------------------\n",
    "project_root = Path(\"/Users/v/Desktop/Visual/glaucoma_DL_project\")\n",
    "vf_path      = project_root / \"data\" / \"uwhvf-master\" / \"CSV\" / \"VF_Data.csv\"\n",
    "coord_path   = project_root / \"data\" / \"uwhvf-master\" / \"CSV\" / \"Coord_242.csv\"\n",
    "out_proc     = project_root / \"data\" / \"processed\"\n",
    "out_feat     = project_root / \"data\" / \"features\"\n",
    "out_proc.mkdir(parents=True, exist_ok=True)\n",
    "out_feat.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Missing-value handling strategy: \"median\" or \"drop\"\n",
    "MISSING_STRATEGY = \"median\"\n",
    "# Optional TD clipping range to reduce extreme outliers\n",
    "TD_CLIP = (-35.0, 5.0)\n",
    "\n",
    "print(\"Loading VF data:\", vf_path)\n",
    "df = pd.read_csv(vf_path)\n",
    "print(\"Loading coordinates:\", coord_path)\n",
    "coords = pd.read_csv(coord_path)\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Identify TD columns & remove blind spots\n",
    "# -----------------------------\n",
    "td_cols = [c for c in df.columns if c.startswith(\"TD_\")]\n",
    "\n",
    "# Blind spots in Coord_242 are Cluster==0; map LocID -> TD column name\n",
    "blind_loc_ids = coords.loc[coords[\"Cluster\"]==0, \"LocID\"].astype(int).tolist()  # typically two locs\n",
    "blind_td_cols = [f\"TD_{lid}\" for lid in blind_loc_ids if f\"TD_{lid}\" in td_cols]\n",
    "\n",
    "print(f\"Detected {len(td_cols)} TD columns total.\")\n",
    "print(f\"Blind-spot TD columns to exclude: {blind_td_cols}\")\n",
    "\n",
    "# Keep only non-blind locations\n",
    "td_keep = [c for c in td_cols if c not in blind_td_cols]\n",
    "L = len(td_keep)  # number of valid locations used (typically 52)\n",
    "print(f\"Using {L} TD locations after removing blind spots.\")\n",
    "\n",
    "# -----------------------------\n",
    "# 3) (Optional) Pre-compute medians for imputation\n",
    "# -----------------------------\n",
    "col_median = df[td_keep].median()  # per-location median for imputing missing values\n",
    "\n",
    "def prepare_vector(row, clip=TD_CLIP, missing=MISSING_STRATEGY):\n",
    "    \"\"\"\n",
    "    Return a TD vector (length L) from a row, with optional clipping & imputation.\n",
    "    \"\"\"\n",
    "    v = row[td_keep].to_numpy(dtype=float)\n",
    "    # Handle missing\n",
    "    if missing == \"median\":\n",
    "        # replace NaN by per-location median\n",
    "        nan_mask = np.isnan(v)\n",
    "        if nan_mask.any():\n",
    "            v[nan_mask] = col_median[nan_mask].to_numpy()\n",
    "    elif missing == \"drop\":\n",
    "        # if any NaN, return None to skip this sample\n",
    "        if np.isnan(v).any():\n",
    "            return None\n",
    "    # Clip extremes\n",
    "    if clip is not None:\n",
    "        v = np.clip(v, clip[0], clip[1])\n",
    "    return v\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Build triplets & assemble X / Y tensors\n",
    "# -----------------------------\n",
    "df = df.sort_values(by=[\"PatID\",\"Eye\",\"FieldN\"]).reset_index(drop=True)\n",
    "\n",
    "X_list, Y_list, meta = [], [], []\n",
    "\n",
    "for (pid, eye), g in df.groupby([\"PatID\",\"Eye\"]):\n",
    "    g = g.sort_values(\"FieldN\")\n",
    "    if len(g) < 3: \n",
    "        continue\n",
    "\n",
    "    for i in range(len(g) - 2):\n",
    "        r1, r2, r3 = g.iloc[i], g.iloc[i+1], g.iloc[i+2]\n",
    "\n",
    "        v1 = prepare_vector(r1)\n",
    "        v2 = prepare_vector(r2)\n",
    "        v3 = prepare_vector(r3)\n",
    "\n",
    "        # If using \"drop\" and any vector returned None, skip\n",
    "        if v1 is None or v2 is None or v3 is None:\n",
    "            continue\n",
    "\n",
    "        # Stack input channels: (2, L); target: (L,)\n",
    "        X = np.stack([v1, v2], axis=0)  # (2, L)\n",
    "        Y = v3                           # (L,)\n",
    "\n",
    "        X_list.append(X)\n",
    "        Y_list.append(Y)\n",
    "        meta.append({\n",
    "            \"PatID\": pid,\n",
    "            \"Eye\": eye,\n",
    "            \"v1_FieldN\": int(r1[\"FieldN\"]),\n",
    "            \"v2_FieldN\": int(r2[\"FieldN\"]),\n",
    "            \"v3_FieldN\": int(r3[\"FieldN\"]),\n",
    "            \"Age_v1\": float(r1[\"Age\"]),\n",
    "            \"Age_v2\": float(r2[\"Age\"]),\n",
    "            \"Age_v3\": float(r3[\"Age\"]),\n",
    "            \"MTD_v1\": float(r1[\"MTD\"]),\n",
    "            \"MTD_v2\": float(r2[\"MTD\"]),\n",
    "            \"MTD_v3\": float(r3[\"MTD\"]),\n",
    "            \"PSD_v1\": float(r1[\"PSD\"]),\n",
    "            \"PSD_v2\": float(r2[\"PSD\"]),\n",
    "            \"PSD_v3\": float(r3[\"PSD\"]),\n",
    "        })\n",
    "\n",
    "X = np.stack(X_list, axis=0)  # (N, 2, L)\n",
    "Y = np.stack(Y_list, axis=0)  # (N, L)\n",
    "meta_df = pd.DataFrame(meta)\n",
    "\n",
    "print(f\"Built X shape: {X.shape}  (N samples, 2 channels, {L} locations)\")\n",
    "print(f\"Built Y shape: {Y.shape}  (N samples, {L} locations)\")\n",
    "print(\"Meta rows:\", meta_df.shape)\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Save arrays and metadata\n",
    "# -----------------------------\n",
    "x_path = out_feat / f\"X_td2ch_L{L}.npy\"\n",
    "y_path = out_feat / f\"Y_td_L{L}.npy\"\n",
    "m_path = out_proc / f\"triplets_meta_L{L}.csv\"\n",
    "\n",
    "np.save(x_path, X)\n",
    "np.save(y_path, Y)\n",
    "meta_df.to_csv(m_path, index=False)\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\"  X ->\", x_path)\n",
    "print(\"  Y ->\", y_path)\n",
    "print(\"  meta ->\", m_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
